{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyricist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlOWSlDt9WT0"
      },
      "source": [
        "# 인공지능 작곡가\n",
        "\n",
        "단어를 입력으로 주면 다음 단어를 예측해 문장을 만들어갑니다.\n",
        "\n",
        "데이터셋 출처 : \n",
        "https://www.kaggle.com/paultimothymooney/poetry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y63VmdUq-xsA"
      },
      "source": [
        "**라이브러리 불러오기 및 난수 고정**\n",
        "\n",
        "가중치값 초기화, shuffle 등의 기능에서 생성된 시드값에 따라 결과가 바뀌곤 합니다. <br/>`tf.random.set_seed()`를 통해 코드를 재생산 할 수 있게합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wjcursq9Enr"
      },
      "source": [
        "# 필요 라이브러리 불러오기\n",
        "import os, glob, re, random\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 결과 복원을 위해 난수 생성 고정\n",
        "\n",
        "tf.random.set_seed(7777)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOiRKyDH-vLH"
      },
      "source": [
        "**파일 읽기**\n",
        "\n",
        "다운받은 가사가 들어있는 폴더를 지정하여 안에있는 파일들을 모두 읽습니다.<br/>\n",
        "이를 한 줄씩 읽어 `raw_corpus` 변수에 저장합니다.<br/>\n",
        "이 과정에서 원치않는 숨김폴더들이 있는 경우 예외처리가 필요할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPju_-vb9HU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a73f75-897d-4ebd-99dd-0c512ca65b28"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/aiffel/ex4/lyrics/*'\n",
        "\n",
        "file_list = glob.glob(file_path)\n",
        "raw_corpus = []\n",
        "\n",
        "for file_ in file_list:\n",
        "    with open(file_, \"r\") as f:\n",
        "        line = f.read().splitlines()\n",
        "        raw_corpus.extend(line)\n",
        "\n",
        "print(\"가사 파일 수 : \", len(file_list))\n",
        "print(\"가사의 줄 수 : \", len(raw_corpus))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "가사 파일 수 :  49\n",
            "가사의 줄 수 :  187088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlxZ1uY3_7CT"
      },
      "source": [
        "**데이터 전처리**\n",
        "\n",
        "한줄 씩 읽은 가사를 학습용 데이터로 변환합니다. 문장의 시작과 끝을 알리는 <start>와 <end> 를 기본으로하고 특수문자들을 처리합니다. 또한 추가적으로 너무 짧은 문장과 너무 긴 문장을 제거했습니다. 마지막으로 문장이 `'('` 로 시작하는 경우 `'['`로 시작하는 경우는 추임새와 후렴구가 포함되는 경우가 많았습니다. 대게의 경우 완성된 문장으로 보기 어렵다 판단하여 제거하였습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkTML-zJ9HS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e00c8a-7b01-423c-993d-5b38f7c7ba66"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # ?.!,¿ 를 만나면 앞뒤를 공백으로 구분합니다.\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 공백이 여러개 있는 부분을 한개로 줄여요\n",
        "    sentence = re.sub(r\"[^a-zA-Z'?.!,¿]+\", \" \", sentence) # a-zA-Z'?.!,¿ 이에 해당되지 않는 또 다른 특수문자를 공백으로 바꿉니다.\n",
        "    sentence = sentence.strip() # 앞뒤 쓸데없는 공백을 제거해요.\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 모델 학습을 위해 문장의 시작을 알리는 <start> 토큰와 끝을 알리는 <end> 토큰을 추가합니다.\n",
        "    return sentence\n",
        "\n",
        "corpus = [] # 전처리 되지 않은 raw_corpus를 읽어서 전처리한 corpus를 저장합니다.\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue # 줄 단위로 raw_corpus에 저장했으니 단순 줄바꿈이 있을 수 있습니다. 제거해줍니다.\n",
        "    if sentence[-1] == \":\": continue # 그룹의 노래일 경우 가수마다 파트가 나눠져있습니다. \"<가수이름> : \" 제거합니다.\n",
        "    if sentence.startswith('('): continue # 추임새 또는 후렴구가 포함되어 있는 경우가 많았습니다.\n",
        "    if sentence.startswith('['): continue # 위와 같음\n",
        "    if len(set(sentence.split())) < 3: continue # 같은 말 반복(Yeah, Yeah, Yeah, Yeah)되거나 I love, 같은 짧은 문장 제거\n",
        "    if len(sentence.split()) > 13: continue # 너무 긴 문장을 제거합니다.\n",
        "\n",
        "    # 위의 조건에 해당되지 않는 경우만 preprocess를 합니다. 굳이 전처리를 할 필요도 없이 제거해버린거죠.\n",
        "        \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "    \n",
        "random.sample(corpus, 5) # 임의의 5래를 출력해봅니다~"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> here are some who like to run . <end>',\n",
              " '<start> black timbs all on your couch again <end>',\n",
              " \"<start> i'm not gonna stand here and wait <end>\",\n",
              " '<start> buying food every once in a while <end>',\n",
              " \"<start> i'm like ooh the girl deserve it , man , nobody so perfect <end>\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6UxpRjxEMrv"
      },
      "source": [
        "**토큰화**\n",
        "\n",
        "앞서 구한 corpus를 통해 텍스트를 여러개의 토큰으로 만듭니다.<br/>\n",
        "전체 사전을 만든다고 이해하면 됩니다. 사전에 담을 단어의 수는 아래와 같이 10000개입니다. 크면 클수록 더 다양한 문장을 만들 수 있습니다. <br/>\n",
        "만든 사전을 기반으로 corpus의 각 문장을 텐서의 형태로 변환합니다. 컴퓨터는 텍스트가 아니라 Tensor의 입력을 처리할 수 있기 때문입니다. 만든 사전에는 단어에 대한 값이 할당되어 있습니다. 이를 통해 `sequences_to_texts` 혹은 `texts_to_sequences`를 수행할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prvmk6wN9HRD"
      },
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=10000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\",\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    return tensor, tokenizer\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOai0oEFlnZ"
      },
      "source": [
        "**X, Y 분리**\n",
        "\n",
        "각 문장을 텐서로 만들었습니다. RNN 학습은 앞서 나온 단어를 입력으로 뒤에 나올 단어를 출력으로 합니다. 시작(`<start>`)부터 마지막(`<end>`)전까지를 입력 데이터로, 시작(`<start>`+1)부터 마지막(`<end>`)까지를 출력 데이터로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YH03LRX9HPE"
      },
      "source": [
        "src_input = tensor[:, :-1]\n",
        "tgt_input = tensor[:, 1:]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTiUdDxGGfE9"
      },
      "source": [
        "**Train, Val** 분리\n",
        "\n",
        "앞서 구한 `X`, `Y`를 train을 위한 데이터, val을 위한 데이터로 쪼갭니다. 여기서 특이한점은 test을 위한 데이터가 없는데, RNN 학습에서는 특별히 test 데아터로 결과를 평가할 수 없기 때문에 나누지 않았다. text generate를 통해 적절한 문장이 생성되었는지 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4bRTeqj9HNR"
      },
      "source": [
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=7777)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI5N_k_1HAa8"
      },
      "source": [
        "데이터들의 형태를 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BSNPKV69HLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8b6568-aa54-4f19-ba8d-76cc06e37725"
      },
      "source": [
        "print(\"encode train:\", enc_train.shape)\n",
        "print(\"decode Train:\", dec_train.shape)\n",
        "print(\"encode val:\", enc_val.shape)\n",
        "print(\"decode val:\", enc_val.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encode train: (120839, 32)\n",
            "decode Train: (120839, 32)\n",
            "encode val: (30210, 32)\n",
            "decode val: (30210, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0AjkZqYHGpR"
      },
      "source": [
        "`TextGenerator` 모델의 구조를 정의한다.\n",
        "\n",
        "- `Embedding`\n",
        "- `LSTM`\n",
        "- `Dropout`\n",
        "- `LSTM`\n",
        "- `Dense`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUk_H6k59HJA"
      },
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size1, hidden_size2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size1, return_sequences=True)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size2, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URilpOIVHkvp"
      },
      "source": [
        "데이터의 전처리가 가장 중요하고 다음으로 모델의 구조가 중요하다. 그 다음으로 중요한 것은 모델을 구성하는 **하이퍼 파라미터**다. <br/>\n",
        "\n",
        "간단하게 `LSTM`레이어의 hidden layer수를 조정하여 모델의 결과를 지켜보고자 하였다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA6Oiq009RHR"
      },
      "source": [
        "# from itertools import product\n",
        "\n",
        "# embedding_size = 256\n",
        "# VOCAB_SIZE = tokenizer.num_words + 1\n",
        "\n",
        "# best_val_loss = 100\n",
        "# best_hidden1 = 0\n",
        "# best_hidden2 = 0\n",
        "\n",
        "# hidden_sizes1 = [1024, 2048, 4096]\n",
        "# hidden_sizes2 = [1024, 2048, 4096]\n",
        "\n",
        "# for hidden_size1, hidden_size2 in product(hidden_sizes1, hidden_sizes2):\n",
        "#     model = TextGenerator(VOCAB_SIZE, embedding_size, hidden_size1, hidden_size2)\n",
        "\n",
        "#     optimizer = tf.keras.optimizers.Adam()\n",
        "#     loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "#         from_logits=True,\n",
        "#         reduction='none'\n",
        "#     )\n",
        "\n",
        "#     model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "#     history = model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val), batch_size=256, epochs=10)\n",
        "#     val_loss = history.history['val_loss'][-1]\n",
        "#     if best_val_loss > val_loss:\n",
        "#         best_val_loss = val_loss\n",
        "#         best_hidden1 = hidden_size1\n",
        "#         best_hidden2 = hidden_size2\n",
        "#     print(\"\\n\"*5)\n",
        "#     print(f\"hidden_size1 : {hidden_size1}, hidden_size2: {hidden_size2}, loss: {val_loss}\")\n",
        "#     print(\"-\"*30)\n",
        "#     print(\"\\n\"*5)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV6PNtvOIqhy"
      },
      "source": [
        "위의 코드를 통해 얻은 결과는 다음과 같다.\n",
        "\n",
        "- [hidden1, hidden2] -> val_loss\n",
        "- [1024, 1024] -> 1.16\n",
        "- [1024, 2048] -> 1.04\n",
        "- [1024, 4096] -> 1.07\n",
        "- [2048, 1024] -> 1.14\n",
        "- [2048, 2048] -> 1.16\n",
        "- [2048, 4096] -> 1.04\n",
        "- [4096, 1024] -> 1.09\n",
        "- [4096, 2048] -> 1.08\n",
        "- [4096, 4096] -> 1.12\n",
        "\n",
        "\n",
        "*지면의 길이가 너무 길어 결과를 추가하지 않았다.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJeI_2xdJ_qu"
      },
      "source": [
        "**가장 좋은 결과를 낸 모델로 다시 모델 학습**\n",
        "\n",
        "위의 결과에서 볼 수 있듯이 [1024, 2048], [2048, 4096]이 좋은 결과를 냈지만 굳이 더 큰 모델을 사용할 필요는 없었다. 따라서 [1024, 2048]을 사용해서 다시 학습한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qtd2JAi9RFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e792b78-8137-4ee6-9712-6014e73433ee"
      },
      "source": [
        "VOCAB_SIZE = tokenizer.num_words + 1\n",
        "embedding_size = 256\n",
        "\n",
        "model = TextGenerator(VOCAB_SIZE, embedding_size, 1024, 2048)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "history = model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val), batch_size=256, epochs=10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "473/473 [==============================] - 200s 392ms/step - loss: 1.6055 - val_loss: 1.4285\n",
            "Epoch 2/10\n",
            "473/473 [==============================] - 184s 390ms/step - loss: 1.3939 - val_loss: 1.3327\n",
            "Epoch 3/10\n",
            "473/473 [==============================] - 185s 390ms/step - loss: 1.2659 - val_loss: 1.2596\n",
            "Epoch 4/10\n",
            "473/473 [==============================] - 184s 390ms/step - loss: 1.1629 - val_loss: 1.1980\n",
            "Epoch 5/10\n",
            "473/473 [==============================] - 184s 390ms/step - loss: 1.0583 - val_loss: 1.1481\n",
            "Epoch 6/10\n",
            "473/473 [==============================] - 185s 390ms/step - loss: 0.9570 - val_loss: 1.1084\n",
            "Epoch 7/10\n",
            "473/473 [==============================] - 185s 390ms/step - loss: 0.8604 - val_loss: 1.0773\n",
            "Epoch 8/10\n",
            "473/473 [==============================] - 185s 390ms/step - loss: 0.7734 - val_loss: 1.0571\n",
            "Epoch 9/10\n",
            "473/473 [==============================] - 185s 390ms/step - loss: 0.6984 - val_loss: 1.0454\n",
            "Epoch 10/10\n",
            "473/473 [==============================] - 185s 390ms/step - loss: 0.6351 - val_loss: 1.0393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ML_Q7nWKJXY"
      },
      "source": [
        "**텍스트를 생성해주는 함수 생성**\n",
        "\n",
        "model, tokenizer, init_sentence, maxlen을 인자로 받아 maxlen에 도달하거나 `<end>` 토큰을 생성할 때까지 문장을 생성합니다.\n",
        "\n",
        "모델에 문장를 토큰화한 Tensor를 입력으로 주면 다음 단어를 예측해서 결과를 줍니다. 단어는 입력 문장과 합쳐집니다. 이를 앞선 조건이 맞을 때까지 반복합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzML4TZq9RDZ"
      },
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=100):\n",
        "    \n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    while True:\n",
        "        predict = model(test_tensor) \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIyef2AH9RBh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74b9eaa0-32eb-4774-e16f-d3af222d1f1d"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i love you so much , so o o o <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ROujpvP9Q_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d25d9797-c9ae-4bf0-a5cf-d99ffc54ab04"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> what\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> what you want be what you want <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9uKgVlrMJnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b7096c3-945f-4dc2-bfe5-d7485a57008d"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> let\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> let me take a ride <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyMr5Vy3MLjc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "492f7f9b-dc54-40d7-aa6d-6265b20ba05c"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> give\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> give me a spanish sweet hoe <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRxZIkKwMNqK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9cd597be-f7b6-4a29-df3c-5693de29283c"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> hello\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> hello hello hello how low <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QArttN4MPMu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "287c4dd1-6a32-4334-a097-5d85305f9c3c"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> kiss\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> kiss me , kiss me <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIiYNOKGMTp3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0de452b9-54ba-44e6-8b77-eee95c784890"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> happy\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> happy , yeah , yeah <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}