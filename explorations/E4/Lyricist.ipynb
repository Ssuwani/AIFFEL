{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlOWSlDt9WT0"
   },
   "source": [
    "# 인공지능 작곡가\n",
    "\n",
    "단어를 입력으로 주면 다음 단어를 예측해 문장을 만들어갑니다.\n",
    "\n",
    "데이터셋 출처 : \n",
    "https://www.kaggle.com/paultimothymooney/poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y63VmdUq-xsA"
   },
   "source": [
    "**라이브러리 불러오기 및 난수 고정**\n",
    "\n",
    "가중치값 초기화, shuffle 등의 기능에서 생성된 시드값에 따라 결과가 바뀌곤 합니다. <br/>`tf.random.set_seed()`를 통해 코드를 재생산 할 수 있게합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-wjcursq9Enr"
   },
   "outputs": [],
   "source": [
    "# 필요 라이브러리 불러오기\n",
    "import os, glob, re, random\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 결과 복원을 위해 난수 생성 고정\n",
    "\n",
    "tf.random.set_seed(7777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOiRKyDH-vLH"
   },
   "source": [
    "**파일 읽기**\n",
    "\n",
    "다운받은 가사가 들어있는 폴더를 지정하여 안에있는 파일들을 모두 읽습니다.<br/>\n",
    "이를 한 줄씩 읽어 `raw_corpus` 변수에 저장합니다.<br/>\n",
    "이 과정에서 원치않는 숨김폴더들이 있는 경우 예외처리가 필요할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KPju_-vb9HU2",
    "outputId": "44a73f75-897d-4ebd-99dd-0c512ca65b28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가사 파일 수 :  49\n",
      "가사의 줄 수 :  187088\n"
     ]
    }
   ],
   "source": [
    "file_path = '/content/drive/MyDrive/aiffel/ex4/lyrics/*'\n",
    "\n",
    "file_list = glob.glob(file_path)\n",
    "raw_corpus = []\n",
    "\n",
    "for file_ in file_list:\n",
    "    with open(file_, \"r\") as f:\n",
    "        line = f.read().splitlines()\n",
    "        raw_corpus.extend(line)\n",
    "\n",
    "print(\"가사 파일 수 : \", len(file_list))\n",
    "print(\"가사의 줄 수 : \", len(raw_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlxZ1uY3_7CT"
   },
   "source": [
    "**데이터 전처리**\n",
    "\n",
    "한줄 씩 읽은 가사를 학습용 데이터로 변환합니다. 문장의 시작과 끝을 알리는 <start>와 <end> 를 기본으로하고 특수문자들을 처리합니다. 또한 추가적으로 너무 짧은 문장과 너무 긴 문장을 제거했습니다. 마지막으로 문장이 `'('` 로 시작하는 경우 `'['`로 시작하는 경우는 추임새와 후렴구가 포함되는 경우가 많았습니다. 대게의 경우 완성된 문장으로 보기 어렵다 판단하여 제거하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkTML-zJ9HS2",
    "outputId": "e6e00c8a-7b01-423c-993d-5b38f7c7ba66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> here are some who like to run . <end>',\n",
       " '<start> black timbs all on your couch again <end>',\n",
       " \"<start> i'm not gonna stand here and wait <end>\",\n",
       " '<start> buying food every once in a while <end>',\n",
       " \"<start> i'm like ooh the girl deserve it , man , nobody so perfect <end>\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # ?.!,¿ 를 만나면 앞뒤를 공백으로 구분합니다.\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 공백이 여러개 있는 부분을 한개로 줄여요\n",
    "    sentence = re.sub(r\"[^a-zA-Z'?.!,¿]+\", \" \", sentence) # a-zA-Z'?.!,¿ 이에 해당되지 않는 또 다른 특수문자를 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 앞뒤 쓸데없는 공백을 제거해요.\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 모델 학습을 위해 문장의 시작을 알리는 <start> 토큰와 끝을 알리는 <end> 토큰을 추가합니다.\n",
    "    return sentence\n",
    "\n",
    "corpus = [] # 전처리 되지 않은 raw_corpus를 읽어서 전처리한 corpus를 저장합니다.\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue # 줄 단위로 raw_corpus에 저장했으니 단순 줄바꿈이 있을 수 있습니다. 제거해줍니다.\n",
    "    if sentence[-1] == \":\": continue # 그룹의 노래일 경우 가수마다 파트가 나눠져있습니다. \"<가수이름> : \" 제거합니다.\n",
    "    if sentence.startswith('('): continue # 추임새 또는 후렴구가 포함되어 있는 경우가 많았습니다.\n",
    "    if sentence.startswith('['): continue # 위와 같음\n",
    "    if len(set(sentence.split())) < 3: continue # 같은 말 반복(Yeah, Yeah, Yeah, Yeah)되거나 I love, 같은 짧은 문장 제거\n",
    "    if len(sentence.split()) > 13: continue # 너무 긴 문장을 제거합니다.\n",
    "\n",
    "    # 위의 조건에 해당되지 않는 경우만 preprocess를 합니다. 굳이 전처리를 할 필요도 없이 제거해버린거죠.\n",
    "        \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "random.sample(corpus, 5) # 임의의 5래를 출력해봅니다~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6UxpRjxEMrv"
   },
   "source": [
    "**토큰화**\n",
    "\n",
    "앞서 구한 corpus를 통해 텍스트를 여러개의 토큰으로 만듭니다.<br/>\n",
    "전체 사전을 만든다고 이해하면 됩니다. 사전에 담을 단어의 수는 아래와 같이 10000개입니다. 크면 클수록 더 다양한 문장을 만들 수 있습니다. <br/>\n",
    "만든 사전을 기반으로 corpus의 각 문장을 텐서의 형태로 변환합니다. 컴퓨터는 텍스트가 아니라 Tensor의 입력을 처리할 수 있기 때문입니다. 만든 사전에는 단어에 대한 값이 할당되어 있습니다. 이를 통해 `sequences_to_texts` 혹은 `texts_to_sequences`를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "prvmk6wN9HRD"
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=10000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\",\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    return tensor, tokenizer\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjOai0oEFlnZ"
   },
   "source": [
    "**X, Y 분리**\n",
    "\n",
    "각 문장을 텐서로 만들었습니다. RNN 학습은 앞서 나온 단어를 입력으로 뒤에 나올 단어를 출력으로 합니다. 시작(`<start>`)부터 마지막(`<end>`)전까지를 입력 데이터로, 시작(`<start>`+1)부터 마지막(`<end>`)까지를 출력 데이터로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3YH03LRX9HPE"
   },
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTiUdDxGGfE9"
   },
   "source": [
    "**Train, Val** 분리\n",
    "\n",
    "앞서 구한 `X`, `Y`를 train을 위한 데이터, val을 위한 데이터로 쪼갭니다. 여기서 특이한점은 test을 위한 데이터가 없는데, RNN 학습에서는 특별히 test 데아터로 결과를 평가할 수 없기 때문에 나누지 않았다. text generate를 통해 적절한 문장이 생성되었는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "s4bRTeqj9HNR"
   },
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=7777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI5N_k_1HAa8"
   },
   "source": [
    "데이터들의 형태를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BSNPKV69HLK",
    "outputId": "ab8b6568-aa54-4f19-ba8d-76cc06e37725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode train: (120839, 32)\n",
      "decode Train: (120839, 32)\n",
      "encode val: (30210, 32)\n",
      "decode val: (30210, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"encode train:\", enc_train.shape)\n",
    "print(\"decode Train:\", dec_train.shape)\n",
    "print(\"encode val:\", enc_val.shape)\n",
    "print(\"decode val:\", enc_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0AjkZqYHGpR"
   },
   "source": [
    "`TextGenerator` 모델의 구조를 정의한다.\n",
    "\n",
    "- `Embedding`\n",
    "- `LSTM`\n",
    "- `Dropout`\n",
    "- `LSTM`\n",
    "- `Dense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JUk_H6k59HJA"
   },
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size1, hidden_size2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size1, return_sequences=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size2, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URilpOIVHkvp"
   },
   "source": [
    "데이터의 전처리가 가장 중요하고 다음으로 모델의 구조가 중요하다. 그 다음으로 중요한 것은 모델을 구성하는 **하이퍼 파라미터**다. <br/>\n",
    "\n",
    "간단하게 `LSTM`레이어의 hidden layer수를 조정하여 모델의 결과를 지켜보고자 하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HA6Oiq009RHR"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "embedding_size = 256\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "best_val_loss = 100\n",
    "best_hidden1 = 0\n",
    "best_hidden2 = 0\n",
    "\n",
    "hidden_sizes1 = [1024, 2048, 4096]\n",
    "hidden_sizes2 = [1024, 2048, 4096]\n",
    "\n",
    "for hidden_size1, hidden_size2 in product(hidden_sizes1, hidden_sizes2):\n",
    "    model = TextGenerator(VOCAB_SIZE, embedding_size, hidden_size1, hidden_size2)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True,\n",
    "        reduction='none'\n",
    "    )\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    history = model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val), batch_size=256, epochs=10)\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    if best_val_loss > val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_hidden1 = hidden_size1\n",
    "        best_hidden2 = hidden_size2\n",
    "    print(\"\\n\"*5)\n",
    "    print(f\"hidden_size1 : {hidden_size1}, hidden_size2: {hidden_size2}, loss: {val_loss}\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"\\n\"*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV6PNtvOIqhy"
   },
   "source": [
    "위의 코드를 통해 얻은 결과는 다음과 같다.\n",
    "\n",
    "- [hidden1, hidden2] -> val_loss\n",
    "- [1024, 1024] -> 1.16\n",
    "- [1024, 2048] -> 1.04\n",
    "- [1024, 4096] -> 1.07\n",
    "- [2048, 1024] -> 1.14\n",
    "- [2048, 2048] -> 1.16\n",
    "- [2048, 4096] -> 1.04\n",
    "- [4096, 1024] -> 1.09\n",
    "- [4096, 2048] -> 1.08\n",
    "- [4096, 4096] -> 1.12\n",
    "\n",
    "\n",
    "*지면의 길이가 너무 길어 결과를 추가하지 않았다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJeI_2xdJ_qu"
   },
   "source": [
    "**가장 좋은 결과를 낸 모델로 다시 모델 학습**\n",
    "\n",
    "위의 결과에서 볼 수 있듯이 [1024, 2048], [2048, 4096]이 좋은 결과를 냈지만 굳이 더 큰 모델을 사용할 필요는 없었다. 따라서 [1024, 2048]을 사용해서 다시 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Qtd2JAi9RFT",
    "outputId": "4e792b78-8137-4ee6-9712-6014e73433ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "473/473 [==============================] - 200s 392ms/step - loss: 1.6055 - val_loss: 1.4285\n",
      "Epoch 2/10\n",
      "473/473 [==============================] - 184s 390ms/step - loss: 1.3939 - val_loss: 1.3327\n",
      "Epoch 3/10\n",
      "473/473 [==============================] - 185s 390ms/step - loss: 1.2659 - val_loss: 1.2596\n",
      "Epoch 4/10\n",
      "473/473 [==============================] - 184s 390ms/step - loss: 1.1629 - val_loss: 1.1980\n",
      "Epoch 5/10\n",
      "473/473 [==============================] - 184s 390ms/step - loss: 1.0583 - val_loss: 1.1481\n",
      "Epoch 6/10\n",
      "473/473 [==============================] - 185s 390ms/step - loss: 0.9570 - val_loss: 1.1084\n",
      "Epoch 7/10\n",
      "473/473 [==============================] - 185s 390ms/step - loss: 0.8604 - val_loss: 1.0773\n",
      "Epoch 8/10\n",
      "473/473 [==============================] - 185s 390ms/step - loss: 0.7734 - val_loss: 1.0571\n",
      "Epoch 9/10\n",
      "473/473 [==============================] - 185s 390ms/step - loss: 0.6984 - val_loss: 1.0454\n",
      "Epoch 10/10\n",
      "473/473 [==============================] - 185s 390ms/step - loss: 0.6351 - val_loss: 1.0393\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "embedding_size = 256\n",
    "\n",
    "model = TextGenerator(VOCAB_SIZE, embedding_size, 1024, 2048)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val), batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ML_Q7nWKJXY"
   },
   "source": [
    "**텍스트를 생성해주는 함수 생성**\n",
    "\n",
    "model, tokenizer, init_sentence, maxlen을 인자로 받아 maxlen에 도달하거나 `<end>` 토큰을 생성할 때까지 문장을 생성합니다.\n",
    "\n",
    "모델에 문장를 토큰화한 Tensor를 입력으로 주면 다음 단어를 예측해서 결과를 줍니다. 단어는 입력 문장과 합쳐집니다. 이를 앞선 조건이 맞을 때까지 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DzML4TZq9RDZ"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=100):\n",
    "    \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dIyef2AH9RBh",
    "outputId": "74b9eaa0-32eb-4774-e16f-d3af222d1f1d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<start> i love you so much , so o o o <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4ROujpvP9Q_P",
    "outputId": "d25d9797-c9ae-4bf0-a5cf-d99ffc54ab04"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<start> what you want be what you want <end> '"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "U9uKgVlrMJnn",
    "outputId": "3b7096c3-945f-4dc2-bfe5-d7485a57008d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<start> let me take a ride <end> '"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> let\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DyMr5Vy3MLjc",
    "outputId": "492f7f9b-dc54-40d7-aa6d-6265b20ba05c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<start> give me a spanish sweet hoe <end> '"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> give\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KRxZIkKwMNqK",
    "outputId": "9cd597be-f7b6-4a29-df3c-5693de29283c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<start> hello hello hello how low <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8QArttN4MPMu",
    "outputId": "287c4dd1-6a32-4334-a097-5d85305f9c3c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<start> kiss me , kiss me <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> kiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "YIiYNOKGMTp3",
    "outputId": "0de452b9-54ba-44e6-8b77-eee95c784890"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<start> happy , yeah , yeah <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**의문점들**\n",
    "\n",
    "- generate_text 함수는 최대 100개의 단어를 가질 수 있도록 만들었는데, 왜 이렇게 짧은 문장으로 끝이난걸까? 입력의 최대 길이를 15로 했기 때문인가? 그렇다면 소설을 쓰는 인공지능은 어떠한 데이터로 학습이 된걸까?\n",
    "- 모델에 입력데이터로 tensor의 (시작: 끝-1), 출력데이터로 tensor (시작+1: 끝) 이렇게 모델에 주었는데, 어떠한 식으로 학습이 되는걸까? 입력한 대로라면 처음부터 끝까지의 입력이 들어가고 마지막 단어만을 예측한다. 그렇다면 마지막 단어만 예측하는 잘못된 입력이라고 생각한다.\n",
    "- batch_size에 따라 학습의 결과가 정말 많이 차이가 났다. GPU의 RAM이 감당할 수 있는 최대의 90% 정도로 BATCH_SIZE를 설정해주면 적절하다고 알고 있었는데 생각과 다른 학습이였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론\n",
    "\n",
    "가장 큰 걸림돌은 시간이였다. 학습을 돌리고 기다리는 시간, 이 시간을 줄일 수 없다면 차라리 길게해서 다른 일에 집중할 수 있도록 코드를 구성해야겠다는 생각이 들었다. 하나의 모델에서 hyperparameter tuning을 값을 변경하며 학습을 하는 과정은 간단하다. 하지만 더 큰 문제는 모델의 구조가 적절한지라고 생각한다. RNN, GRU, LSTM, Bi-LSTM 등 다양한 기법들이 있지만 이를 어떻게 조합해야 하는지 아는것은 더욱 중요한 과제이다. 각각의 레이어들이 어떠한 방식으로 동작하는 지 데이터에 따른 어떠한 레이어가 적절한지에 대해 더 공부하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Lyricist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
